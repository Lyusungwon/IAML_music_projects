{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from tensorflow.python.ops.rnn_cell import BasicRNNCell, BasicLSTMCell, GRUCell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.platform import gfile\n",
    "from time import strftime, localtime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Extraction function\n",
    "\n",
    "def get_features(tids):\n",
    "    feature_list = []\n",
    "    try:\n",
    "        for n, tid in enumerate(tids):\n",
    "            tid, features= compute_features(tid)\n",
    "            feature_list.append(features)\n",
    "            print(\"Extracted features audio track\", n)\n",
    "    except Exception as e:\n",
    "        print('{}: {}'.format(tid, repr(e)))\n",
    "\n",
    "    return np.array(feature_list)\n",
    "\n",
    "def compute_features(tid):\n",
    "    threshold = 1278900\n",
    "    timeseries_length = 2498\n",
    "    hop_length = 512\n",
    "    try:\n",
    "        filepath = get_audio_path('music/music_training', tid)\n",
    "        x, sr = librosa.load(filepath, sr=None, mono=True, duration=29.0)  # kaiser_fast\n",
    "        x = x.tolist()\n",
    "        if(len(x) < threshold):\n",
    "            raise ValueError('song length is shorter than threshold')\n",
    "        else:\n",
    "            x = x[:int(1278900)]#131000\n",
    "        x = np.array(x)\n",
    "        data = np.zeros((timeseries_length, 33), dtype=np.float64)\n",
    "        mfcc = librosa.feature.mfcc(x, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "        spectral_center = librosa.feature.spectral_centroid(x, sr=sr, hop_length=hop_length)\n",
    "        chroma = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(x, sr=sr, hop_length=hop_length)\n",
    "        data[:, 0:13] = mfcc.T[0:timeseries_length, :]\n",
    "        data[:, 13:14] = spectral_center.T[0:timeseries_length, :]\n",
    "        data[:, 14:26] = chroma.T[0:timeseries_length, :]\n",
    "        data[:, 26:33] = spectral_contrast.T[0:timeseries_length, :]\n",
    "\n",
    "    except Exception as e:\n",
    "        print('{}: {}'.format(tid, repr(e)))\n",
    "        return tid, 0\n",
    "\n",
    "    return tid, data\n",
    "\n",
    "def get_audio_path(audio_dir, track_id):\n",
    "    tid_str = '{:06d}'.format(track_id)\n",
    "    return os.path.join(audio_dir, tid_str[:3], tid_str + '.mp3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3995, 499, 33), (31905, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract Training Sets\n",
    "\n",
    "metadata_path = 'dataset/track_metadata.csv'\n",
    "label_column_name = 'listens'\n",
    "is_train_mode = True\n",
    "label_dict = {'low': 0,\n",
    " 'medium': 1,\n",
    " 'high': 2}\n",
    "\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "if is_train_mode:\n",
    "    metadata_df = metadata_df[metadata_df['set_split'] == 'training']\n",
    "else:\n",
    "    metadata_df = metadata_df[metadata_df['set_split'] == 'validation']\n",
    "track_ids = np.array(metadata_df['track_id'])\n",
    "tn = len(track_ids)\n",
    "mix = get_features(track_ids)\n",
    "\n",
    "########IMPORTANT############ Modify Training Sets  \n",
    "\n",
    "split = 5\n",
    "n = mix.shape[0]\n",
    "h = mix.shape[1]//split\n",
    "w = mix.shape[2]\n",
    "mixe = np.zeros((n*split, h, w))\n",
    "for i in range(split):\n",
    "    mixe[i*n:(i+1)*n, :, :h] = mix[:,i*h:(i+1)*h,:]\n",
    "label_array = np.zeros((metadata_df.shape[0]*split, len(label_dict)))\n",
    "labels = metadata_df[label_column_name].values\n",
    "for j in range(split):\n",
    "    for i, label in enumerate(labels):\n",
    "        label_pos = label_dict.get(label)\n",
    "        label_array[j*tn + i, label_pos] = 1\n",
    "ys = label_array.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save as pkl\n",
    "with open('mixe.pkl', 'wb') as mixf:\n",
    "    pickle.dump(mixe, mixf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('ye.pkl', 'wb') as yf:\n",
    "    pickle.dump(ys, yf, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Training Sets\n",
    "\n",
    "class MacOSFile(object):\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            pos = 0\n",
    "            while pos < n:\n",
    "                size = min(n - pos, 1 << 31 - 1)\n",
    "                chunk = self.f.read(size)\n",
    "                buffer[pos:pos + size] = chunk\n",
    "                pos += size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "with open(\"mixe.pkl\", 'rb') as mixf:\n",
    "\tXd = pickle.load(MacOSFile(mixf))\n",
    "with open(\"ye.pkl\", 'rb') as yf:\n",
    "\tyd = pickle.load(yf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3995, 499, 33), (3995, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############Extract Validation/Test Sets###############\n",
    "\n",
    "is_train_mode = False\n",
    "\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "if is_train_mode:\n",
    "    metadata_df = metadata_df[metadata_df['set_split'] == 'training']\n",
    "else:\n",
    "    metadata_df = metadata_df[metadata_df['set_split'] == 'validation'] ######################## Modify here to insert test index\n",
    "track_ids_val = np.array(metadata_df['track_id'])\n",
    "vn = len(track_ids_val)\n",
    "mix_val = get_features(track_ids_val)\n",
    "########IMPORTANT############ Modify Training Sets\n",
    "split = 5\n",
    "n = mix_val.shape[0]\n",
    "h = mix_val.shape[1]//split\n",
    "w = mix_val.shape[2]\n",
    "mixe_val = np.zeros((n*split, h, w))\n",
    "for i in range(split):\n",
    "    mixe_val[i*n:(i+1)*n, :, :h] = mix_val[:,i*h:(i+1)*h,:]\n",
    "label_array_val = np.zeros((metadata_df.shape[0]*split, len(label_dict)))\n",
    "labels_val = metadata_df[label_column_name].values\n",
    "for j in range(split):\n",
    "    for i, label in enumerate(labels_val):\n",
    "        label_pos = label_dict.get(label)\n",
    "        label_array_val[j*vn + i, label_pos] = 1\n",
    "ys_val = label_array_val.copy()\n",
    "mixe_val.shape, ys_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save as pkl\n",
    "with open('mixe_val.pkl', 'wb') as mix_valf:\n",
    "    pickle.dump(mixe_val, mix_valf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('ye_val.pkl', 'wb') as y_valf:\n",
    "    pickle.dump(ys_val, y_valf, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Test Sets\n",
    "\n",
    "with open('mixe_val.pkl', 'rb') as mix_valf:\n",
    "\tXd_val = pickle.load(mix_valf)\n",
    "with open('ye_val.pkl', 'rb') as y_valf:\n",
    "\tyd_val = pickle.load(y_valf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for training and validation\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_input = 33 * 499\n",
    "n_classs = 3\n",
    "image_height = 33\n",
    "timesteps = 499\n",
    "# rnn property\n",
    "num_hidden = 250\n",
    "num_layers = 2\n",
    "\n",
    "# Placeholder and variables\n",
    "# TODO : declare placeholder and variables\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, timesteps, image_height])\n",
    "y = tf.placeholder(tf.int64, [None, n_classs])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# Build model\n",
    "# TODO : build your model here\n",
    "# Model\n",
    "\n",
    "def model(X,y,is_training):\n",
    "    w_out = tf.get_variable(\"W_out\", shape=[num_hidden, n_classs])\n",
    "    b_out = tf.get_variable(\"b_out\", shape=[n_classs])\n",
    "\n",
    "    x = tf.unstack(X, timesteps, 1)    \n",
    "    stack_rnn = []\n",
    "    for i in range(num_layers):\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.8)\n",
    "        stack_rnn.append(cell)\n",
    "    stacked_cell = tf.nn.rnn_cell.MultiRNNCell(stack_rnn, state_is_tuple=True)\n",
    "\n",
    "    outputs, last_states = rnn.static_rnn(stacked_cell, x, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.matmul(outputs[-1], w_out) + b_out\n",
    "    return logits\n",
    "\n",
    "y_out = model(X,y,is_training)\n",
    "\n",
    "# Loss and optimizer\n",
    "# TODO : declare loss and optimizer operation\n",
    "\n",
    "total_loss = tf.losses.softmax_cross_entropy(y,logits=y_out) \n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) \n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)    \n",
    "correct_prediction = tf.equal(tf.argmax(y_out,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/mix5_rnn_2_250\n"
     ]
    }
   ],
   "source": [
    "# properties\n",
    "# General\n",
    "# TODO : declare additional properties\n",
    "# not fixed (change or add property as you like)\n",
    "batch_size = 128\n",
    "epoch_num = 5\n",
    "print_every = 10\n",
    "\n",
    "# fixed\n",
    "metadata_path = 'dataset/track_metadata.csv'\n",
    "# True if you want to train, False if you already trained your model\n",
    "# TODO : IMPORTANT !!! Please change it to False when you submit your code\n",
    "is_train_mode = False\n",
    "validation = True\n",
    "# TODO : IMPORTANT !!! Please specify the path where your best model is saved\n",
    "# example : checkpoint/run-0925-0348\n",
    "checkpoint_path = 'checkpoint/mix5_rnn_2_250'\n",
    "\n",
    "# X, y, mean_loss,correct_prediction,train_step, accuracy = my_model(lr = 2e-4, rl = 1e-4,  is_training= is_train_mode)\n",
    "#load data\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, checkpoint_path)\t\t\t\n",
    "    if is_train_mode:\n",
    "        variables = [mean_loss,correct_prediction,train_step]\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "        iter_cnt = 0\n",
    "        for e in range(epoch_num):              \n",
    "            correct = 0\n",
    "            losses = []\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx] }\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "                loss, corr, _ = sess.run(variables,feed_dict=feed_dict)\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "                if is_train_mode and (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "        print('Training finished !')\n",
    "#         output_dir = checkpoint_path + '/run-%02d%02d-%02d%02d' % tuple(localtime(time()))[1:5]\n",
    "#         output_dir = checkpoint_path\n",
    "#         if not gfile.Exists(output_dir):\n",
    "#             gfile.MakeDirs(output_dir)\n",
    "#         saver.save(sess, output_dir)\n",
    "#         print('Model saved in file : %s' % output_dir)\n",
    "\n",
    "    if validation:\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        preds = np.zeros([1,3])\n",
    "        train_indicies = np.arange(Xd_val.shape[0])\n",
    "        variables = [mean_loss,correct_prediction,y_out]\n",
    "        for j in range(int(math.ceil(Xd_val.shape[0]/batch_size))):\n",
    "            start_idx = (j*batch_size)%Xd_val.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            feed_dict_val = {X: Xd_val[idx, :],\n",
    "                         y: yd_val[idx] }\n",
    "            actual_batch_size = yd_val[idx].shape[0]\n",
    "            loss, corr, predict = sess.run(variables,feed_dict=feed_dict_val)\n",
    "            preds = np.concatenate((preds, predict), axis = 0)\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "        votes = np.zeros([Xd_val.shape[0]//split, 3])\n",
    "        for n, i in enumerate(preds[1:]):\n",
    "#             votes[n%(Xd_val.shape[0]//split),np.argmax(i)] += 1\n",
    "            votes[n%(Xd_val.shape[0]//split),:] += i\n",
    "        total_val_correct = correct/Xd_val.shape[0]\n",
    "        total_val_loss = np.sum(losses)/Xd_val.shape[0]\n",
    "        sum_total_val_correct = sum(((np.argmax(votes, axis = 1) == np.argmax(yd_val[:Xd_val.shape[0]//split], axis = 1))))/(Xd_val.shape[0]//split)\n",
    "        print(\"Validation loss, Overall loss = {0:.3g}, accuracy of {1:.3g}\"\\\n",
    "          .format(total_val_loss,sum_total_val_correct))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
